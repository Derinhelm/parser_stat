{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11295150,
          "sourceType": "datasetVersion",
          "datasetId": 7062743
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Derinhelm/parser_stat/blob/llm_taiga/udeppllamarunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code downloading"
      ],
      "metadata": {
        "id": "vQpB65N5z79d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Derinhelm/parser_stat.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFCWmTwmA_j2",
        "outputId": "58693ffe-47d1-41fe-f983-879847aaf891"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'parser_stat'...\n",
            "remote: Enumerating objects: 312, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 312 (delta 36), reused 45 (delta 17), pack-reused 231 (from 1)\u001b[K\n",
            "Receiving objects: 100% (312/312), 53.53 MiB | 17.47 MiB/s, done.\n",
            "Resolving deltas: 100% (163/163), done.\n",
            "Updating files: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/parser_stat')\n",
        "\n",
        "from data_classes import ConllEntry, Sentence"
      ],
      "metadata": {
        "id": "OBw7SusfKovW",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:37:56.368189Z",
          "iopub.execute_input": "2025-05-05T13:37:56.368565Z",
          "iopub.status.idle": "2025-05-05T13:37:56.384551Z",
          "shell.execute_reply.started": "2025-05-05T13:37:56.368536Z",
          "shell.execute_reply": "2025-05-05T13:37:56.383661Z"
        }
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing"
      ],
      "metadata": {
        "id": "3rCMCNnD0iDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "Jf-9gGT99CdG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:37:56.385459Z",
          "iopub.execute_input": "2025-05-05T13:37:56.385686Z",
          "iopub.status.idle": "2025-05-05T13:37:56.388771Z",
          "shell.execute_reply.started": "2025-05-05T13:37:56.385666Z",
          "shell.execute_reply": "2025-05-05T13:37:56.388066Z"
        }
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import traceback"
      ],
      "metadata": {
        "id": "4LpFrgGGT0TQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:37:56.389860Z",
          "iopub.execute_input": "2025-05-05T13:37:56.390135Z",
          "iopub.status.idle": "2025-05-05T13:37:56.409017Z",
          "shell.execute_reply.started": "2025-05-05T13:37:56.390107Z",
          "shell.execute_reply": "2025-05-05T13:37:56.408194Z"
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "from data_classes import ConllEntry, Sentence\n",
        "\n",
        "def get_dataset_sentences(dataset_path):\n",
        "    fh = open(dataset_path,'r',encoding='utf-8')\n",
        "    sents_read = 0\n",
        "    sents = []\n",
        "    comments = set()\n",
        "\n",
        "    sent = Sentence()\n",
        "    for line in fh:\n",
        "        tok = line.strip().split('\\t')\n",
        "        if not tok or line.strip() == '': # empty line, add sentence to list\n",
        "            if sent.is_not_empty:\n",
        "                sents_read += 1\n",
        "                sents.append(sent)\n",
        "            sent = Sentence()\n",
        "        else:\n",
        "            if line[0] == '#' or '-' in tok[0]: # a comment line\n",
        "                line = line.strip()\n",
        "                if line[:12] == \"# sent_id = \":\n",
        "                    sent.set_sent_id(line[12:])\n",
        "                elif line[:9] == \"# text = \":\n",
        "                    sent.set_text(line[9:])\n",
        "                else:\n",
        "                    comments.add(line)\n",
        "\n",
        "            else: # an actual ConllEntry, add to tokens\n",
        "                if tok[2] == \"_\":\n",
        "                    tok[2] = tok[1].lower()\n",
        "\n",
        "                word = ConllEntry(*tok)\n",
        "                sent.add_token(word)\n",
        "    fh.close()\n",
        "    return sents"
      ],
      "metadata": {
        "id": "Y_mOKFO2K5PP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treebank_name = 'taiga'\n",
        "taiga_data = get_dataset_sentences(f\"/content/parser_stat/treebank_test_sets/ru_{treebank_name}-ud-test.conllu\")\n",
        "print(treebank_name, len(taiga_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4iUTNgAK81i",
        "outputId": "fafd766e-e7f3-4505-9484-22a4cea5ffce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "taiga 881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "taiga_data[1].text"
      ],
      "metadata": {
        "id": "5GVJpnKaLNuS",
        "outputId": "2c5f7ec9-1871-493f-ddf0-e12b8967ad59",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:37:57.452319Z",
          "iopub.execute_input": "2025-05-05T13:37:57.452556Z",
          "iopub.status.idle": "2025-05-05T13:37:57.457726Z",
          "shell.execute_reply.started": "2025-05-05T13:37:57.452536Z",
          "shell.execute_reply": "2025-05-05T13:37:57.457085Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Она решила попытаться остановить машину — хотя выйдя под дождь, сразу же промокла насквозь.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UDepPLLaMA running"
      ],
      "metadata": {
        "id": "uZUulp-k5rlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft transformers bitsandbytes"
      ],
      "metadata": {
        "id": "iy87eoje6BXS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:37:57.458673Z",
          "iopub.execute_input": "2025-05-05T13:37:57.458876Z",
          "iopub.status.idle": "2025-05-05T13:38:04.472999Z",
          "shell.execute_reply.started": "2025-05-05T13:37:57.458852Z",
          "shell.execute_reply": "2025-05-05T13:38:04.472192Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "9eShUZEC3sTR",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:38:04.473967Z",
          "iopub.execute_input": "2025-05-05T13:38:04.474186Z",
          "iopub.status.idle": "2025-05-05T13:38:24.916500Z",
          "shell.execute_reply.started": "2025-05-05T13:38:04.474167Z",
          "shell.execute_reply": "2025-05-05T13:38:24.915820Z"
        }
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "OP = '['\n",
        "CP = ']'\n",
        "\n",
        "class UDepPLLaMAParser:\n",
        "    def __init__(self):\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        model_from = AutoModelForCausalLM.from_pretrained(\n",
        "            \"NousResearch/Llama-2-7b-hf\",\n",
        "            #load_in_4bit=True,\n",
        "            quantization_config=quant_config,\n",
        "            torch_dtype=torch.float16,\n",
        "            trust_remote_code=True,\n",
        "            device_map={\"\": 0},\n",
        "        )\n",
        "\n",
        "        model = PeftModel.from_pretrained(\n",
        "            model_from,\n",
        "            \"sag-uniroma2/u-depp-llama-2-7b\"\n",
        "        )\n",
        "\n",
        "        generation_config = GenerationConfig(\n",
        "            num_beams=4,\n",
        "            do_sample=False,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\", trust_remote_code=True)\n",
        "        self.model = model\n",
        "        self.generation_config = generation_config\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "\n",
        "    def get_llm_output(self, input):\n",
        "        prompt = f\"\"\"\n",
        "        ### Input:\n",
        "        {input}\n",
        "        ### Answer:\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        input_ids = inputs[\"input_ids\"].to(self.model.device)\n",
        "        with torch.no_grad():\n",
        "            gen_outputs = self.model.generate(\n",
        "                input_ids=input_ids,\n",
        "                generation_config=self.generation_config,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                max_new_tokens=1024,\n",
        "                use_cache=True,\n",
        "            )\n",
        "        s = gen_outputs.sequences[0]\n",
        "        output = self.tokenizer.decode(s, skip_special_tokens=True)\n",
        "\n",
        "        response = output.split(\"### Answer:\")[1].rstrip().lstrip()\n",
        "        #print(response)\n",
        "        return response\n",
        "\n",
        "    def parseExpression(self, expression):\n",
        "        nodeMap = dict()\n",
        "        counter = 1\n",
        "        node = \"\"\n",
        "        retExp =\"\"\n",
        "        for char in expression:\n",
        "            if char == OP or char == CP :\n",
        "                if (len(node) > 0):\n",
        "                    nodeMap[str(counter)] = node;\n",
        "                    retExp += str(counter)\n",
        "                    counter +=1\n",
        "                retExp += char\n",
        "                node =\"\"\n",
        "            elif char == ' ': continue\n",
        "            else :\n",
        "                node += char\n",
        "        return retExp,nodeMap\n",
        "\n",
        "    def toTree(self, expression):\n",
        "        tree = dict()\n",
        "        msg =\"\"\n",
        "        stack = list()\n",
        "        for char in expression:\n",
        "            if(char == OP):\n",
        "                stack.append(msg)\n",
        "                msg = \"\"\n",
        "            elif char == CP:\n",
        "                parent = stack.pop()\n",
        "                if parent not in tree:\n",
        "                    tree[parent] = list()\n",
        "                tree[parent].append(msg)\n",
        "                msg = parent\n",
        "            else:\n",
        "                msg += char\n",
        "        return tree\n",
        "\n",
        "\n",
        "    def _decode(self, tree, representation_type, node, nodeMap, parent, grand_parent, tid2treenodeMap, res):\n",
        "        if node not in tree:\n",
        "            tid = 1\n",
        "            if res:\n",
        "                tid = int(max(res.keys())) + 1\n",
        "\n",
        "            grand_parent_label = \"ROOT\"\n",
        "            if grand_parent in nodeMap:\n",
        "                grand_parent_label = nodeMap[grand_parent]\n",
        "\n",
        "            if representation_type == \"lct\":\n",
        "                res[tid] = { \"id\": tid, \"form\": nodeMap[parent], \"to\": grand_parent_label, \"toid\" : grand_parent, \"deprel\": nodeMap[node] }\n",
        "            elif representation_type == \"grct\":\n",
        "                res[tid] = { \"id\": tid, \"form\": nodeMap[node], \"to\": grand_parent_label, \"toid\" : grand_parent, \"deprel\": nodeMap[parent] }\n",
        "            else:\n",
        "                raise Exception(\"The representation_type\\t\" + representation_type + \"\\t is not supported in decoding.\")\n",
        "\n",
        "            tid2treenodeMap[parent] = str(tid)\n",
        "\n",
        "            return\n",
        "\n",
        "        for child in tree[node]:\n",
        "            self._decode(tree, representation_type, child, nodeMap, node, parent, tid2treenodeMap, res)\n",
        "\n",
        "    def decode(self, tree, nodeMap, representation_type=\"lct\"):\n",
        "        res = dict()\n",
        "        tid2treenodeMap = dict()\n",
        "        #print(tree[''][0])\n",
        "        self._decode(tree, representation_type, \"1\", nodeMap, None, None, tid2treenodeMap, res)\n",
        "\n",
        "        for i in range(1, len(res)+1):\n",
        "            if res[i][\"toid\"] is None:\n",
        "                res[i][\"toid\"] = '0'\n",
        "            else:\n",
        "                try:\n",
        "                    res[i][\"toid\"] = tid2treenodeMap[res[i][\"toid\"]]\n",
        "                except:\n",
        "                    res[i][\"toid\"] = '0'\n",
        "\n",
        "        return res\n",
        "\n",
        "    def _parse(self, s):\n",
        "        llm_output = self.get_llm_output(s)\n",
        "        retExp, nodeMap = self.parseExpression(llm_output)\n",
        "        tree = self.toTree(retExp)\n",
        "        res = self.decode(tree, nodeMap)\n",
        "        return res\n",
        "\n",
        "    def parse(self, sent):\n",
        "        parsing_res = self._parse(sent)\n",
        "        res = []\n",
        "        for token in parsing_res.values():\n",
        "          t =  { 'id': str(token['id']), 'form': token['deprel'],\n",
        "                 'parent_id': token['toid'], 'relation': token['form'] }\n",
        "          res.append(t)\n",
        "        return res"
      ],
      "metadata": {
        "id": "hlEYTA-E3sTR",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:38:24.917283Z",
          "iopub.execute_input": "2025-05-05T13:38:24.917880Z",
          "iopub.status.idle": "2025-05-05T13:38:24.932151Z",
          "shell.execute_reply.started": "2025-05-05T13:38:24.917848Z",
          "shell.execute_reply": "2025-05-05T13:38:24.931475Z"
        }
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "parser = UDepPLLaMAParser()\n"
      ],
      "metadata": {
        "id": "9XQ2oQuyK0ox",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:38:24.934122Z",
          "iopub.execute_input": "2025-05-05T13:38:24.934319Z",
          "iopub.status.idle": "2025-05-05T13:41:19.347947Z",
          "shell.execute_reply.started": "2025-05-05T13:38:24.934301Z",
          "shell.execute_reply": "2025-05-05T13:41:19.346252Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ts = time.time()\n",
        "parser.parse(\"Мама мыла раму.\")\n",
        "print(time.time() - ts)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:41:19.349076Z",
          "iopub.execute_input": "2025-05-05T13:41:19.349327Z",
          "iopub.status.idle": "2025-05-05T13:41:34.291793Z",
          "shell.execute_reply.started": "2025-05-05T13:41:19.349305Z",
          "shell.execute_reply": "2025-05-05T13:41:34.291136Z"
        },
        "id": "XVavhMRWA8sv",
        "outputId": "81c8ff4b-96f1-43e7-a941-fc16ab2643ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17.610638856887817\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "zzX_YA7NK46J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#taiga_data = taiga_data[:5] # Uncomment for testing on small dataset version"
      ],
      "metadata": {
        "id": "nzO8tlkVLVVd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "start_i = 0\n",
        "finish_i = len(taiga_data)\n",
        "\n",
        "t_res = {}\n",
        "print(\"\\n\", treebank_name)\n",
        "t_time = []\n",
        "for i in range(start_i, finish_i):\n",
        "    if i % 20 == 0:\n",
        "        with open(f\"{treebank_name}_{start_i}_{i}.pickle\", 'wb') as f:\n",
        "            pickle.dump(t_res, f)\n",
        "        gc.collect()\n",
        "    sent = taiga_data[i]\n",
        "    ts = time.time()\n",
        "    try:\n",
        "        token_list = parser.parse(sent.text)\n",
        "    except Exception as err:\n",
        "        t_res[i] = (err, )\n",
        "        print(i, err)\n",
        "    else:\n",
        "        te = time.time()\n",
        "        t_time.append(te - ts)\n",
        "        cur_res = Sentence()\n",
        "        cur_res.set_sent_id(sent.sent_id)\n",
        "        cur_res.set_text(sent.text)\n",
        "        for t in token_list:\n",
        "            cur_res.add_token(t)\n",
        "        t_res[i] = (cur_res, t_time[-1])\n",
        "        print(i, t_time[-1])\n",
        "\n",
        "all_time = sum(t_time)\n",
        "\n",
        "print(f\"Time: {all_time:5.3f} (s)\")\n",
        "\n",
        "with open(treebank_name + \".pickle\", 'wb') as f:\n",
        "    pickle.dump(t_res, f)"
      ],
      "metadata": {
        "id": "dKm9MqX-K0rW",
        "outputId": "02204c4f-d364-4881-b115-74b0b5d739a4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-05T13:42:56.100730Z",
          "iopub.execute_input": "2025-05-05T13:42:56.101048Z",
          "iopub.status.idle": "2025-05-05T16:48:40.212065Z",
          "shell.execute_reply.started": "2025-05-05T13:42:56.101015Z",
          "shell.execute_reply": "2025-05-05T16:48:40.211221Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " taiga\n",
            "0 69.96101880073547\n",
            "1 69.03670978546143\n",
            "2 34.33293128013611\n",
            "3 58.96249794960022\n",
            "4 5.033560752868652\n",
            "Time: 237.327 (s)\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dFTgzUL7M9P9"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}