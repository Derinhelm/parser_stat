{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Derinhelm/parser_stat/blob/main/parser_running.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQpB65N5z79d"
      },
      "source": [
        "# Code downloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_GSFZQ1rd9m",
        "outputId": "76f2a782-b7c9-44e1-aa9d-50835adf95a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'parser_stat'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 63 (delta 30), reused 36 (delta 13), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (63/63), 14.17 MiB | 13.71 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b llm-parser https://github.com/Derinhelm/parser_stat.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwS2uPLzrp8l"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/parser_stat')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rCMCNnD0iDV"
      },
      "source": [
        "# Preparing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jf-9gGT99CdG"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF95eNWm9CWL"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LpFrgGGT0TQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import traceback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZUulp-k5rlG"
      },
      "source": [
        "# UDepPLLaMA running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy87eoje6BXS"
      },
      "outputs": [],
      "source": [
        "!pip install peft transformers bitsandbytes\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "from data_classes import ConllEntry, Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "from peft import PeftModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OP = '['\n",
        "CP = ']'\n",
        "\n",
        "class UDepPLLaMAParser:\n",
        "    def __init__(self):\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        model_from = AutoModelForCausalLM.from_pretrained(\n",
        "            \"NousResearch/Llama-2-13b-hf\",\n",
        "            #load_in_4bit=True,\n",
        "            quantization_config=quant_config,\n",
        "            torch_dtype=torch.float16,\n",
        "            trust_remote_code=True,\n",
        "            device_map={\"\": 0},\n",
        "        )\n",
        "\n",
        "        model = PeftModel.from_pretrained(\n",
        "            model_from,\n",
        "            \"sag-uniroma2/u-depp-llama-2-13b\"\n",
        "        )\n",
        "\n",
        "        generation_config = GenerationConfig(\n",
        "            num_beams=4,\n",
        "            do_sample=False,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-13b-hf\", trust_remote_code=True)\n",
        "        self.model = model\n",
        "        self.generation_config = generation_config\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "\n",
        "    def get_llm_output(self, input):\n",
        "        prompt = f\"\"\"\n",
        "        ### Input:\n",
        "        {input}\n",
        "        ### Answer:\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        input_ids = inputs[\"input_ids\"].to(self.model.device)\n",
        "        with torch.no_grad():\n",
        "            gen_outputs = self.model.generate(\n",
        "                input_ids=input_ids,\n",
        "                generation_config=self.generation_config,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                max_new_tokens=1024,\n",
        "                use_cache=True,\n",
        "            )\n",
        "        s = gen_outputs.sequences[0]\n",
        "        output = self.tokenizer.decode(s, skip_special_tokens=True)\n",
        "\n",
        "        response = output.split(\"### Answer:\")[1].rstrip().lstrip()\n",
        "        #print(response)\n",
        "\n",
        "        del input_ids\n",
        "        torch.cuda.empty_cache()\n",
        "        return response\n",
        "\n",
        "    def parseExpression(self, expression):\n",
        "        nodeMap = dict()\n",
        "        counter = 1\n",
        "        node = \"\"\n",
        "        retExp =\"\"\n",
        "        for char in expression:\n",
        "            if char == OP or char == CP :\n",
        "                if (len(node) > 0):\n",
        "                    nodeMap[str(counter)] = node;\n",
        "                    retExp += str(counter)\n",
        "                    counter +=1\n",
        "                retExp += char\n",
        "                node =\"\"\n",
        "            elif char == ' ': continue\n",
        "            else :\n",
        "                node += char\n",
        "        return retExp,nodeMap\n",
        "\n",
        "    def toTree(self, expression):\n",
        "        tree = dict()\n",
        "        msg =\"\"\n",
        "        stack = list()\n",
        "        for char in expression:\n",
        "            if(char == OP):\n",
        "                stack.append(msg)\n",
        "                msg = \"\"\n",
        "            elif char == CP:\n",
        "                parent = stack.pop()\n",
        "                if parent not in tree:\n",
        "                    tree[parent] = list()\n",
        "                tree[parent].append(msg)\n",
        "                msg = parent\n",
        "            else:\n",
        "                msg += char\n",
        "        return tree\n",
        "\n",
        "\n",
        "    def _decode(self, tree, representation_type, node, nodeMap, parent, grand_parent, tid2treenodeMap, res):\n",
        "        if node not in tree:\n",
        "            tid = 1\n",
        "            if res:\n",
        "                tid = int(max(res.keys())) + 1\n",
        "\n",
        "            grand_parent_label = \"ROOT\"\n",
        "            if grand_parent in nodeMap:\n",
        "                grand_parent_label = nodeMap[grand_parent]\n",
        "\n",
        "            if representation_type == \"lct\":\n",
        "                res[tid] = { \"id\": tid, \"form\": nodeMap[parent], \"to\": grand_parent_label, \"toid\" : grand_parent, \"deprel\": nodeMap[node] }\n",
        "            elif representation_type == \"grct\":\n",
        "                res[tid] = { \"id\": tid, \"form\": nodeMap[node], \"to\": grand_parent_label, \"toid\" : grand_parent, \"deprel\": nodeMap[parent] }\n",
        "            else:\n",
        "                raise Exception(\"The representation_type\\t\" + representation_type + \"\\t is not supported in decoding.\")\n",
        "\n",
        "            tid2treenodeMap[parent] = str(tid)\n",
        "\n",
        "            return\n",
        "\n",
        "        for child in tree[node]:\n",
        "            self._decode(tree, representation_type, child, nodeMap, node, parent, tid2treenodeMap, res)\n",
        "\n",
        "    def decode(self, tree, nodeMap, representation_type=\"lct\"):\n",
        "        res = dict()\n",
        "        tid2treenodeMap = dict()\n",
        "        self._decode(tree, representation_type, \"1\", nodeMap, None, None, tid2treenodeMap, res)\n",
        "\n",
        "        for i in range(1, len(res)+1):\n",
        "            if res[i][\"toid\"] is None:\n",
        "                res[i][\"toid\"] = '0'\n",
        "            else:\n",
        "                try:\n",
        "                    res[i][\"toid\"] = tid2treenodeMap[res[i][\"toid\"]]\n",
        "                except:\n",
        "                    res[i][\"toid\"] = '0'\n",
        "\n",
        "        return res\n",
        "\n",
        "    def _parse(self, s):\n",
        "        llm_output = self.get_llm_output(s)\n",
        "        retExp, nodeMap = self.parseExpression(llm_output)\n",
        "        tree = self.toTree(retExp)\n",
        "        res = self.decode(tree, nodeMap)\n",
        "        return res\n",
        "\n",
        "    def parse(self, sent):\n",
        "        parsing_res = self._parse(sent)\n",
        "        res = []\n",
        "        for token in parsing_res.values():\n",
        "          t =  ConllEntry(str(token['id']), form=token['deprel'], parent_id=token['toid'], relation=token['form'])\n",
        "          res.append(t)\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "pickle_data_path = \"/content/parser_stat/treebank_test_sets/treebank_data.pickle\"\n",
        "\n",
        "with open(pickle_data_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "\n",
        "res = {}\n",
        "time_dict = {}\n",
        "parser = UDepPLLaMAParser()\n",
        "\n",
        "for treebank_name, treebank_sents in data.items():\n",
        "    t_res = []\n",
        "    print(\"\\n\", treebank_name)\n",
        "    t_time = []\n",
        "    for i, sent in enumerate(treebank_sents):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"{i:4}/{len(treebank_sents)}\")\n",
        "        try:\n",
        "            ts = time.time()\n",
        "            token_list = parser.parse(sent.text)\n",
        "            te = time.time()\n",
        "            t_time.append(te - ts)\n",
        "            \n",
        "            cur_res = Sentence()\n",
        "            cur_res.set_sent_id(sent.sent_id)\n",
        "            cur_res.set_text(sent.text)\n",
        "            for t in token_list:\n",
        "                cur_res.add_token(t)\n",
        "            t_res.append(cur_res)\n",
        "        except Exception as e:\n",
        "            t_res.append((e, traceback.format_exc()))\n",
        "    res[treebank_name] = t_res\n",
        "    time_dict[treebank_name] = sum(t_time)\n",
        "\n",
        "print(\"\\ntime results (s):\")\n",
        "for p, t in time_dict.items():\n",
        "    print(f\"{p:10}: {t:5.3f} (s)\")\n",
        "\n",
        "with open(f'udeppllama.pickle', 'wb') as f:\n",
        "    pickle.dump(res, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "aHNcddoA6CH5",
        "outputId": "088a87ad-15aa-46e5-a521-197826b4f0f7"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_e5c63106-b790-43e9-8e75-fdd119943fcd\", \"udpipe.pickle\", 15452693)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download(\"/content/udeppllama.pickle\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMj0sKs+1/xs4iTnejLte6/",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
